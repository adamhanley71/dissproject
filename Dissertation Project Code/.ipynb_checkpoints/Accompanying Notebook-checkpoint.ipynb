{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to accompany the paper \"CREATING A SCORE INDEPENDENT PERFORMANCE MEASURE FOR FOOTBALL MATCHES, USING NEURAL NETWORKS\". It contains the code used to preprocess the data, create the datasets, train the models and evaluate them. The datasets and the models used in the paper can be loaded in this notebook. If the code to train the model is executed it may not produce the same model produced in the paper, due to the inherent stochasticity of the training algorithms used. The exact models produced and referenced in the paper can be loaded from file, and used to evaluate the data in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Libraries Used</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries used in this notebook are listed and imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Pre-processing </h1>\n",
    "\n",
    "The data for this project was provided in several large JSON files. In order to use the data, it first had to be converted into the appropriate form. This only needed to be done once, and the pre-processed data could then be saved, in this case as CSV files. Due to the size of the raw data files, only the processed data is supplied with this notebook. The original JSON files can be downloaded [here](https://figshare.com/collections/Soccer_match_event_dataset/4415000/5). The code below will not run without these data files, but is provided for reference. The processed data can be loaded, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-075189bfa123>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmatches_england\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matches/matches_england.json'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#The Json files are loaded using Pandas.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmatches_spain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matches/matches_spain.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmatches_germany\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matches/matches_germany.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmatches_italy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matches/matches_italy.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmatches_france\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matches/matches_france.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"series\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1091\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m-> 1093\u001b[1;33m                 \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m             )\n\u001b[0;32m   1095\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "matches_england=pd.read_json('matches/matches_england.json') #The Json files are loaded using Pandas.\n",
    "matches_spain=pd.read_json('matches/matches_spain.json')\n",
    "matches_germany=pd.read_json('matches/matches_germany.json')\n",
    "matches_italy=pd.read_json('matches/matches_italy.json')\n",
    "matches_france=pd.read_json('matches/matches_france.json')\n",
    "matches = pd.concat([matches_england,matches_spain, matches_germany,matches_italy,matches_france], ignore_index=True) #The matches and events for different competitions are stored in separate files, and so must be concatenated together.\n",
    "events_england = pd.read_json('events/events_england.json')\n",
    "events_spain = pd.read_json('events/events_Spain.json')\n",
    "events_Germany = pd.read_json('events/events_Germany.json')\n",
    "events_Italy = pd.read_json('events/events_Italy.json')\n",
    "events_france = pd.read_json('events/events_France.json')\n",
    "events = pd.concat([events_england,events_spain,events_Germany,events_Italy,events_france],ignore_index=True)\n",
    "detailedStats = events.subEventId.unique() #An array of the different event and subevent ID's is needed to build the statistics array for each match.\n",
    "simpleStats = events.eventId.unique()\n",
    "def getDetailedMatchStats(x): #This function, when given a match number, returns an array with the count for each type event from the detailed array. If this event did not occur in the match, the array will contain a 0 for that entry.\n",
    "    hometeam,awayteam=getTeamIds(x) #The ID's for the teams in the match are provided here.\n",
    "    matchId = matches.iloc[x]['wyId'] #Pandas is used to get the specific match ID, an internal identifier used within the data sets, as opposed to the match number, which is an index. \n",
    "    homeTeamStats = events[(events['matchId']==matchId)&(events['teamId'] == hometeam)].groupby('subEventId').count()['tags'] #Pandas allows for specific, efficient filtering of data sets. This code provides the \n",
    "    awayTeamStats = events[(events['matchId']==matchId)&(events['teamId'] == awayteam)].groupby('subEventId').count()['tags'] #counts of each sub event type for a given team in this match.\n",
    "    matchStats = []\n",
    "    for i in detailedStats: #The array of possible stats is iterated through and the count is placed in the array. \n",
    "        matchStats.append(homeTeamStats.get(i,0)) #The home team stats are provided first, then the away team.\n",
    "        matchStats.append(awayTeamStats.get(i,0))\n",
    "    return np.asarray(matchStats) #The array is returned as a numpy array.\n",
    "\n",
    "def getSimpleMatchStats(x): #The exact same process can be done with the simple stats array, to provide statistics for the simple model.\n",
    "    hometeam,awayteam=getTeamIds(x)\n",
    "    matchId = matches.iloc[x]['wyId']\n",
    "    homeTeamStats = events[(events['matchId']==matchId)&(events['teamId'] == hometeam)].groupby('eventId').count()['tags']\n",
    "    awayTeamStats = events[(events['matchId']==matchId)&(events['teamId'] == awayteam)].groupby('eventId').count()['tags']\n",
    "    matchStats = []\n",
    "    for i in simpleStats:\n",
    "        matchStats.append(homeTeamStats.get(i,0))\n",
    "        matchStats.append(awayTeamStats.get(i,0))\n",
    "    return np.asarray(matchStats)\n",
    "\n",
    "def getTeamIds(x): #This function returns the Team IDs for the teams that took part in a given match.\n",
    "    homeId, awayId = matches.iloc[x]['teamsData']\n",
    "    return int(homeId),int(awayId)\n",
    "\n",
    "def getScore(x): #This function gets the score for a given match, used to determine the class labels for each match.\n",
    "    homeId,awayId = matches.iloc[x]['teamsData']\n",
    "    homeId,awayId = int(homeId),int(awayId)\n",
    "    h,a = matches.iloc[x]['teamsData'].values()\n",
    "    return h['score'],a['score']\n",
    "\n",
    "simpleStatsArray = []\n",
    "detailedStatsArray = []\n",
    "scores = []\n",
    "for i in range(0,int(matches.shape[0])):\n",
    "    simpleStatsArray.append(getSimpleMatchStats(i))\n",
    "    detailedStatsArray.append(getDetailedMatchStats(i))\n",
    "    scores.append(getScore(i))\n",
    "\n",
    "scores = np.asarray(scores) \n",
    "homewin = scores[:,0]>scores[:,1] #A class label is determined by comparing the scores of the two teams.\n",
    "draw = scores[:,0]==scores[:,1]\n",
    "awaywin = scores[:,0]<scores[:,1] \n",
    "result = np.stack([homewin,draw,awaywin],axis=1) #The labesl are stacked into a single array.\n",
    "result = result*1 #This is converted to a one hot array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once run, the above code provided 3 arrays, two different statistics arrays and the class label array. These were then saved to CSV files and can be loaded below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleStatsArray = pd.read_csv(\"simpleStatsArray.csv\")\n",
    "simpleStatsArray = np.asarray(simpleStatsArray)\n",
    "simpleStatsArray = simpleStatsArray[:,1:]\n",
    "detailedStatsArray = pd.read_csv('detailedStatsArray.csv')\n",
    "detailedStatsArray = np.asarray(detailedStatsArray)\n",
    "detailedStatsArray = detailedStatsArray[:,1:]\n",
    "results = pd.read_csv('results.csv')\n",
    "results = np.asarray(results)\n",
    "results = results[:,1:]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building a Model</h1>\n",
    "With the data loaded, we can build and train models. The code for building a model is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layer1, layer2, layer3): #The function takes 3 parameters, the number of nodes on each of the hidden layers.\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer1,input_dim=20, activation='relu')) #The input layer needs to match the dimensionality of the data set.\n",
    "    model.add(Dense(layer2, activation = 'relu'))\n",
    "    model.add(Dense(layer3, activation = 'relu'))\n",
    "    model.add(Dense(3,activation='softmax')) #The output layer has 3 nodes, one for each class label. Softmax outputs a vector of categotical probabilities\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code returns a model that can be fit to the data set, and validated against an unseen portion of it.\n",
    "The models used in the paper have been provided and so can be loaded from a file, so it is unnecessary to refit a model. This code is included for completeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1460 samples, validate on 366 samples\n",
      "Epoch 1/50\n",
      "1460/1460 [==============================] - 4s 3ms/sample - loss: 6.9359 - accuracy: 0.3110 - val_loss: 1.6673 - val_accuracy: 0.3087\n",
      "Epoch 2/50\n",
      "1460/1460 [==============================] - 1s 740us/sample - loss: 1.3829 - accuracy: 0.3404 - val_loss: 1.2020 - val_accuracy: 0.3142\n",
      "Epoch 3/50\n",
      "1460/1460 [==============================] - 1s 729us/sample - loss: 1.2275 - accuracy: 0.3575 - val_loss: 1.3872 - val_accuracy: 0.3087\n",
      "Epoch 4/50\n",
      "1460/1460 [==============================] - 1s 725us/sample - loss: 1.1604 - accuracy: 0.3925 - val_loss: 1.1591 - val_accuracy: 0.4098\n",
      "Epoch 5/50\n",
      "1460/1460 [==============================] - 1s 732us/sample - loss: 1.1399 - accuracy: 0.3781 - val_loss: 1.1599 - val_accuracy: 0.3579\n",
      "Epoch 6/50\n",
      "1460/1460 [==============================] - 1s 726us/sample - loss: 1.1198 - accuracy: 0.4014 - val_loss: 1.1911 - val_accuracy: 0.2869\n",
      "Epoch 7/50\n",
      "1460/1460 [==============================] - 1s 716us/sample - loss: 1.0954 - accuracy: 0.4089 - val_loss: 1.1435 - val_accuracy: 0.2760\n",
      "Epoch 8/50\n",
      "1460/1460 [==============================] - 1s 724us/sample - loss: 1.1025 - accuracy: 0.4027 - val_loss: 1.0932 - val_accuracy: 0.4590\n",
      "Epoch 9/50\n",
      "1460/1460 [==============================] - 1s 723us/sample - loss: 1.1037 - accuracy: 0.4089 - val_loss: 1.0943 - val_accuracy: 0.5055\n",
      "Epoch 10/50\n",
      "1460/1460 [==============================] - 1s 719us/sample - loss: 1.0608 - accuracy: 0.4603 - val_loss: 1.0703 - val_accuracy: 0.4098\n",
      "Epoch 11/50\n",
      "1460/1460 [==============================] - 1s 688us/sample - loss: 1.0461 - accuracy: 0.4692 - val_loss: 1.0485 - val_accuracy: 0.5191\n",
      "Epoch 12/50\n",
      "1460/1460 [==============================] - 1s 692us/sample - loss: 1.0291 - accuracy: 0.4932 - val_loss: 1.0307 - val_accuracy: 0.5164\n",
      "Epoch 13/50\n",
      "1460/1460 [==============================] - 1s 721us/sample - loss: 1.0091 - accuracy: 0.5240 - val_loss: 1.0144 - val_accuracy: 0.5464\n",
      "Epoch 14/50\n",
      "1460/1460 [==============================] - 1s 702us/sample - loss: 0.9838 - accuracy: 0.5342 - val_loss: 1.0424 - val_accuracy: 0.5219\n",
      "Epoch 15/50\n",
      "1460/1460 [==============================] - 1s 716us/sample - loss: 0.9742 - accuracy: 0.5315 - val_loss: 1.0964 - val_accuracy: 0.4672\n",
      "Epoch 16/50\n",
      "1460/1460 [==============================] - 1s 695us/sample - loss: 0.9570 - accuracy: 0.5397 - val_loss: 1.1055 - val_accuracy: 0.4399\n",
      "Epoch 17/50\n",
      "1460/1460 [==============================] - 1s 727us/sample - loss: 0.9661 - accuracy: 0.5397 - val_loss: 1.3423 - val_accuracy: 0.3470\n",
      "Epoch 18/50\n",
      "1460/1460 [==============================] - 1s 701us/sample - loss: 0.9418 - accuracy: 0.5603 - val_loss: 0.9577 - val_accuracy: 0.5710\n",
      "Epoch 19/50\n",
      "1460/1460 [==============================] - 1s 723us/sample - loss: 0.9115 - accuracy: 0.5774 - val_loss: 1.0386 - val_accuracy: 0.4781\n",
      "Epoch 20/50\n",
      "1460/1460 [==============================] - 1s 696us/sample - loss: 0.9086 - accuracy: 0.5767 - val_loss: 0.9236 - val_accuracy: 0.5492\n",
      "Epoch 21/50\n",
      "1460/1460 [==============================] - 1s 697us/sample - loss: 0.9258 - accuracy: 0.5616 - val_loss: 0.9413 - val_accuracy: 0.5601\n",
      "Epoch 22/50\n",
      "1460/1460 [==============================] - 1s 701us/sample - loss: 0.9012 - accuracy: 0.5795 - val_loss: 0.9363 - val_accuracy: 0.5492\n",
      "Epoch 23/50\n",
      "1460/1460 [==============================] - 1s 709us/sample - loss: 0.8987 - accuracy: 0.5890 - val_loss: 0.9161 - val_accuracy: 0.5683\n",
      "Epoch 24/50\n",
      "1460/1460 [==============================] - 1s 690us/sample - loss: 0.8957 - accuracy: 0.5952 - val_loss: 0.9388 - val_accuracy: 0.5574\n",
      "Epoch 25/50\n",
      "1460/1460 [==============================] - 1s 697us/sample - loss: 0.8954 - accuracy: 0.5904 - val_loss: 0.9300 - val_accuracy: 0.5437\n",
      "Epoch 26/50\n",
      "1460/1460 [==============================] - 1s 697us/sample - loss: 0.8968 - accuracy: 0.5829 - val_loss: 0.9448 - val_accuracy: 0.5519\n",
      "Epoch 27/50\n",
      "1460/1460 [==============================] - 1s 705us/sample - loss: 0.8881 - accuracy: 0.5897 - val_loss: 0.9440 - val_accuracy: 0.5546\n",
      "Epoch 28/50\n",
      "1460/1460 [==============================] - 1s 718us/sample - loss: 0.8737 - accuracy: 0.5932 - val_loss: 0.8902 - val_accuracy: 0.5765\n",
      "Epoch 29/50\n",
      "1460/1460 [==============================] - 1s 714us/sample - loss: 0.8737 - accuracy: 0.6082 - val_loss: 0.9404 - val_accuracy: 0.5301\n",
      "Epoch 30/50\n",
      "1460/1460 [==============================] - 1s 719us/sample - loss: 0.8767 - accuracy: 0.5979 - val_loss: 1.0438 - val_accuracy: 0.4863\n",
      "Epoch 31/50\n",
      "1460/1460 [==============================] - 1s 731us/sample - loss: 0.8694 - accuracy: 0.5945 - val_loss: 0.8995 - val_accuracy: 0.5437\n",
      "Epoch 32/50\n",
      "1460/1460 [==============================] - 1s 731us/sample - loss: 0.8757 - accuracy: 0.5966 - val_loss: 0.9365 - val_accuracy: 0.5410\n",
      "Epoch 33/50\n",
      "1460/1460 [==============================] - 1s 735us/sample - loss: 0.8735 - accuracy: 0.6055 - val_loss: 0.8778 - val_accuracy: 0.5902\n",
      "Epoch 34/50\n",
      "1460/1460 [==============================] - 1s 723us/sample - loss: 0.8577 - accuracy: 0.6055 - val_loss: 0.8943 - val_accuracy: 0.5874\n",
      "Epoch 35/50\n",
      "1460/1460 [==============================] - 1s 733us/sample - loss: 0.8474 - accuracy: 0.6110 - val_loss: 0.8759 - val_accuracy: 0.5847\n",
      "Epoch 36/50\n",
      "1460/1460 [==============================] - 1s 722us/sample - loss: 0.8569 - accuracy: 0.6048 - val_loss: 0.9303 - val_accuracy: 0.5574\n",
      "Epoch 37/50\n",
      "1460/1460 [==============================] - 1s 730us/sample - loss: 0.8535 - accuracy: 0.6164 - val_loss: 0.9013 - val_accuracy: 0.5464\n",
      "Epoch 38/50\n",
      "1460/1460 [==============================] - 1s 739us/sample - loss: 0.8537 - accuracy: 0.6075 - val_loss: 0.9352 - val_accuracy: 0.5437\n",
      "Epoch 39/50\n",
      "1460/1460 [==============================] - 1s 736us/sample - loss: 0.8751 - accuracy: 0.6041 - val_loss: 0.8822 - val_accuracy: 0.5874\n",
      "Epoch 40/50\n",
      "1460/1460 [==============================] - 1s 726us/sample - loss: 0.8541 - accuracy: 0.5973 - val_loss: 0.8705 - val_accuracy: 0.5765\n",
      "Epoch 41/50\n",
      "1460/1460 [==============================] - 1s 727us/sample - loss: 0.8580 - accuracy: 0.6151 - val_loss: 0.8882 - val_accuracy: 0.5628\n",
      "Epoch 42/50\n",
      "1460/1460 [==============================] - 1s 727us/sample - loss: 0.8411 - accuracy: 0.6103 - val_loss: 0.8856 - val_accuracy: 0.5464\n",
      "Epoch 43/50\n",
      "1460/1460 [==============================] - 1s 740us/sample - loss: 0.8393 - accuracy: 0.6158 - val_loss: 0.9921 - val_accuracy: 0.5219\n",
      "Epoch 44/50\n",
      "1460/1460 [==============================] - 1s 731us/sample - loss: 0.8518 - accuracy: 0.6062 - val_loss: 0.9042 - val_accuracy: 0.5574\n",
      "Epoch 45/50\n",
      "1460/1460 [==============================] - 1s 726us/sample - loss: 0.8423 - accuracy: 0.6171 - val_loss: 0.8759 - val_accuracy: 0.5710\n",
      "Epoch 46/50\n",
      "1460/1460 [==============================] - 1s 782us/sample - loss: 0.8389 - accuracy: 0.6219 - val_loss: 0.9132 - val_accuracy: 0.5464\n",
      "Epoch 47/50\n",
      "1460/1460 [==============================] - 1s 789us/sample - loss: 0.8292 - accuracy: 0.6226 - val_loss: 0.8945 - val_accuracy: 0.5902\n",
      "Epoch 48/50\n",
      "1460/1460 [==============================] - 1s 828us/sample - loss: 0.8410 - accuracy: 0.6103 - val_loss: 0.8950 - val_accuracy: 0.5956\n",
      "Epoch 49/50\n",
      "1460/1460 [==============================] - 1s 730us/sample - loss: 0.8520 - accuracy: 0.6110 - val_loss: 0.9256 - val_accuracy: 0.5109\n",
      "Epoch 50/50\n",
      "1460/1460 [==============================] - 1s 721us/sample - loss: 0.8394 - accuracy: 0.6247 - val_loss: 0.8772 - val_accuracy: 0.5656\n"
     ]
    }
   ],
   "source": [
    "model = create_model(8,8,8) #A model is created with given parameters.\n",
    "log_dir = os.path.join('logs','fit',datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) #This string is used to indentify the model in TensorBoard.\n",
    "tensorboard_callback = TensorBoard(log_dir = log_dir, histogram_freq = 1)\n",
    "history = model.fit(simpleStatsArray, result,validation_split = 0.2,epochs = 50, batch_size = 5, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the development process I trained 200 models for each data set and selected the one with the highest validation accuracy. These models can be loaded from the file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleClassifier = load_model('simple_classifier.h5')\n",
    "detailedClassifier = load_model('detailed_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models achieved the highest accuracy and so were used to create the results detailed in the document. Below they are used to predict the outcome of all the matches in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Using the Models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplePerformanceMeasure = simpleClassifier.predict(simpleStatsArray)\n",
    "detailedPerformanceMeasure = detailedClassifier.predict(detailedStatsArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 2 arrays contain the likelihood of each result, home win, draw or away win, for each of the matches in the data set. The results used in the paper are printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.7984177e-04 9.9972016e-01 1.4970114e-09]\n",
      " [7.4925506e-01 6.5303335e-05 2.5067961e-01]\n",
      " [6.4440101e-02 2.7205525e-02 9.0835434e-01]\n",
      " [4.5432900e-03 3.4453182e-03 9.9201137e-01]\n",
      " [9.9792693e-08 4.6926198e-04 9.9953067e-01]\n",
      " [2.6539459e-15 1.0149865e-19 1.0000000e+00]\n",
      " [9.9998164e-01 2.4300002e-06 1.5938405e-05]\n",
      " [9.9999940e-01 6.5054746e-07 2.9664842e-14]\n",
      " [1.4606271e-08 3.2505763e-05 9.9996746e-01]\n",
      " [1.4052362e-02 2.4408686e-01 7.4186081e-01]\n",
      " [1.0000000e+00 6.8397127e-13 7.1703768e-17]]\n",
      "\n",
      "[[2.4452115e-28 1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.8910593e-16 0.0000000e+00]\n",
      " [9.9999988e-01 1.4424349e-08 1.7084355e-07]\n",
      " [1.0546842e-06 9.9999893e-01 4.1038444e-32]\n",
      " [2.2733140e-08 9.9999559e-01 4.3534988e-06]\n",
      " [1.9548748e-15 4.7113371e-09 1.0000000e+00]\n",
      " [1.0000000e+00 1.9427962e-13 3.3124564e-13]\n",
      " [1.0000000e+00 3.0273557e-14 4.6374581e-26]\n",
      " [3.5002199e-07 9.9999881e-01 8.0431340e-07]\n",
      " [7.5528832e-25 4.7761464e-13 1.0000000e+00]\n",
      " [1.0000000e+00 1.0371099e-15 8.4952149e-12]]\n"
     ]
    }
   ],
   "source": [
    "indices = [35,33,55,854,938,1087,1215,467,506,1556,1595] #The indices of the matches used in the test set.\n",
    "print(np.take(simplePerformanceMeasure,indices,axis=0))\n",
    "print()\n",
    "print(np.take(detailedPerformanceMeasure,indices,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jupyter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0247df9357ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjupyter\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'jupyter' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
